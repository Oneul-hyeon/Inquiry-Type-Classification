{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzzI2w2HDh8O",
        "outputId": "36ecee85-7ff2-4d7b-a86e-9d9a73efa56e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Inquiry-Type-Classification'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 37 (delta 15), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (37/37), 804.21 KiB | 6.14 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "/content/Inquiry-Type-Classification\n",
            "Collecting kobert_tokenizer (from -r requirements.txt (line 11))\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-tw95t2jv/kobert-tokenizer_ebf24aff07c946be98f22b1d1e5153a0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-tw95t2jv/kobert-tokenizer_ebf24aff07c946be98f22b1d1e5153a0\n",
            "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3<=1.15.18 (from -r requirements.txt (line 1))\n",
            "  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gluonnlp==0.8.0 (from -r requirements.txt (line 2))\n",
            "  Downloading gluonnlp-0.8.0.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mxnet (from -r requirements.txt (line 3))\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime==1.12.0 (from -r requirements.txt (line 4))\n",
            "  Downloading onnxruntime-1.12.0-cp310-cp310-manylinux_2_27_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.96 (from -r requirements.txt (line 5))\n",
            "  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.0.1+cu118)\n",
            "Collecting transformers (from -r requirements.txt (line 7))\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.66.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.8.0->-r requirements.txt (line 2)) (1.23.5)\n",
            "Collecting coloredlogs (from onnxruntime==1.12.0->-r requirements.txt (line 4))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0->-r requirements.txt (line 4)) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0->-r requirements.txt (line 4)) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0->-r requirements.txt (line 4)) (1.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (4.7.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 6)) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 6)) (16.0.6)\n",
            "Collecting botocore<1.19.0,>=1.18.18 (from boto3<=1.15.18->-r requirements.txt (line 1))\n",
            "  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3<=1.15.18->-r requirements.txt (line 1))\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3<=1.15.18->-r requirements.txt (line 1))\n",
            "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet->-r requirements.txt (line 3)) (2.31.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet->-r requirements.txt (line 3))\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers->-r requirements.txt (line 7))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 7))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 7))\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 9)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 9)) (2023.3)\n",
            "Collecting urllib3<1.26,>=1.20 (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->-r requirements.txt (line 1))\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers->-r requirements.txt (line 7)) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 9)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet->-r requirements.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet->-r requirements.txt (line 3)) (2023.7.22)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.12.0->-r requirements.txt (line 4))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 6)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime==1.12.0->-r requirements.txt (line 4)) (1.3.0)\n",
            "Building wheels for collected packages: gluonnlp, kobert_tokenizer\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.8.0-py3-none-any.whl size=292697 sha256=1236b678498774f03a024e9828eedeed01f7b1fac45015335f77141060148d8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/cc/dc/7ec84dced25f738b8be400101abb67e4b50c905090a51017e4\n",
            "  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4632 sha256=1404ae8680541012d1055685be8ab2ce343abe38431297e4a95cce064371b9ab\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nwrwf2k7/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n",
            "Successfully built gluonnlp kobert_tokenizer\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, kobert_tokenizer, urllib3, jmespath, humanfriendly, graphviz, gluonnlp, coloredlogs, botocore, s3transfer, onnxruntime, mxnet, huggingface-hub, transformers, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.1\n",
            "    Uninstalling graphviz-0.20.1:\n",
            "      Successfully uninstalled graphviz-0.20.1\n",
            "Successfully installed boto3-1.15.18 botocore-1.18.18 coloredlogs-15.0.1 gluonnlp-0.8.0 graphviz-0.8.4 huggingface-hub-0.16.4 humanfriendly-10.0 jmespath-0.10.0 kobert_tokenizer-0.1 mxnet-1.9.1 onnxruntime-1.12.0 s3transfer-0.3.7 safetensors-0.3.3 sentencepiece-0.1.96 tokenizers-0.13.3 transformers-4.32.0 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Oneul-hyeon/Inquiry-Type-Classification.git\n",
        "%cd Inquiry-Type-Classification\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.환경설정"
      ],
      "metadata": {
        "id": "KOfy2djLD4Nu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) 라이브러리 불러오기"
      ],
      "metadata": {
        "id": "LFWBOiw4HUMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import joblib\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "c3SkSJqaHa96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) 데이터 불러오기"
      ],
      "metadata": {
        "id": "3MuAP3iiHPRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/Inquiry-Type-Classification/dataset.csv')"
      ],
      "metadata": {
        "id": "j2r5bD-nHTOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.데이터 전처리"
      ],
      "metadata": {
        "id": "JK3F1-O-EDkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) 개행 문자 제거"
      ],
      "metadata": {
        "id": "RRoEpS7C5IPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'] = data['text'].str.replace(r'\\n','',regex = True)"
      ],
      "metadata": {
        "id": "hX3uOnReEknk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) 불용어 처리"
      ],
      "metadata": {
        "id": "G3sxICvs5gp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한국어 불용어 데이터"
      ],
      "metadata": {
        "id": "gnzwF26N6Yl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 불용어 처리에 필요한 데이터 다운로드\n",
        "sys.stdin = open('/content/Inquiry-Type-Classification/stopwords.txt','r')\n",
        "input = sys.stdin.readline\n",
        "stopwords_ko = []\n",
        "while True :\n",
        "    x = input().rstrip()\n",
        "    if x == '' : break\n",
        "    else :\n",
        "        stopwords_ko.append(x)"
      ],
      "metadata": {
        "id": "Ibo1MKVj5iHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 영어 불용어 데이터"
      ],
      "metadata": {
        "id": "LX8Q8inD6gOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 영어 불용어 처리에 필요한 데이터 다운로드\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stopwords_en = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCCN6FuB6haZ",
        "outputId": "bde6a648-f30d-4265-ff85-467f149b1684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 불용어 처리"
      ],
      "metadata": {
        "id": "caw3-rM19VY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = data['text']\n",
        "\n",
        "# 불용어 처리된 텍스트 리스트\n",
        "filtered_texts = []\n",
        "\n",
        "# 텍스트별 불용어 처리\n",
        "for text in texts:\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if (word not in stopwords_ko) and (word not in stopwords_en)]\n",
        "    # 불용어 처리된 텍스트를 공백으로 결합해 리스트에 추가\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    filtered_texts.append(filtered_text)\n",
        "data['text'] = filtered_texts"
      ],
      "metadata": {
        "id": "JXgZ_3N-9Wy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (3) Label 값 변경"
      ],
      "metadata": {
        "id": "2vyYePkCBdw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {\n",
        "    '코드1': 0,\n",
        "    '코드2': 0,\n",
        "    '웹': 1,\n",
        "    '이론': 2,\n",
        "    '시스템 운영': 3,\n",
        "    '원격': 4\n",
        "}"
      ],
      "metadata": {
        "id": "r5UTQrphBW71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['label'] = data.label.apply(lambda t : label_dict[t])"
      ],
      "metadata": {
        "id": "OAbv0gLeBmlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 데이터 파일로 저장"
      ],
      "metadata": {
        "id": "1ACTzTtRCM6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(data, '/content/Inquiry-Type-Classification/preprocessed_dataset.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uCR5OqECNJ-",
        "outputId": "46be85fc-c7af-431d-bf09-fd7db6df5a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Inquiry-Type-Classification/preprocessed_dataset.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}